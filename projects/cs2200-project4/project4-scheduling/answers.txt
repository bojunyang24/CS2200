CS 2200 Spring 2018
Project 4

Name: Bojun Yang
GT Username: byang301

Problem 1B
----------

CPU: 1
# of Context Switches: 97
Total execution time: 67.8 s
Total time spent in READY state: 389.5 s

CPU: 2
# of Context Switches: 107
Total execution time: 36.0 s
Total time spent in READY state: 78.1 s

CPU: 4
# of Context Switches: 184
Total execution time: 33.4 s
Total time spent in READY state: 0.8 s

There is not a linear relationship. The trend seems like exponential decay.
The increase to 2 cpus shows a large decrease in execution time because the cpus can do work in parallel.
The effect is less evident when we increase the number of cpus to 4 because any program will contain parts of work that must be done serially.

Problem 2B
----------

1CPU, 800ms
# of Context Switches: 135
Total execution time: 67.6 s
Total time spent in READY state: 328.1 s

1CPU, 600ms
# of Context Switches: 160
Total execution time: 67.6 s
Total time spent in READY state: 314.6 s

1CPU, 400ms
# of Context Switches: 202
Total execution time: 67.8 s
Total time spent in READY state: 301.4 s

1CPU, 200ms
# of Context Switches: 362
Total execution time: 67.7 s
Total time spent in READY state: 287.8 s

In a real OS, the shortest timeslice is usually not the best choice because the CPU will be too busy switching between processes to get any work done.

Problem 3B
----------

1CPU
# of Context Switches: 161
Total execution time: 67.8 s
Total time spent in READY state: 122.8 s

2CPUs
# of Context Switches: 174
Total execution time: 38.6 s
Total time spent in READY state: 23.5 s

4CPUs
# of Context Switches: 181
Total execution time: 34.2 s
Total time spent in READY state: 0.1 s

A STRF is often impossible to implement precisely because in a real operating system we don't always know exactly how much time remains in a process.
It is simpler to implement priority because we can easily manipulate the priority of a process since it's an extrinsic property.
Priority Scheduling has the lowest wait times because the higher priority processes have the shortest CPU bursts with long IO bursts while lower priority processes have higher CPU bursts but lower IO bursts. Using priority scheduling in this case is the most efficient way since the high priority processes will finish their quick CPU bursts and move into their slower IO bursts. This give the lower priority processes time to run their longer CPU bursts so that when they finish the higher priority processes will have just about finished IO bursts too.
